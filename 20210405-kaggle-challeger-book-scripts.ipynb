{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the new scripts for the challenge book //Jung Kweon Woo\n",
    "# some competition notebook codes explanations\n",
    "# 1. check if a competition has data leakage (in notebooks/discussions)\n",
    "# 2. top kaggler that submit below 200\n",
    "# spec: tabular data, standard ML pipline\n",
    "# RAM 32GB, CPU 8core\n",
    "# if the data is larger than mem, then use online learning (Vowpal Wabbit)\n",
    "# spec: image/audio, DL, GPU RAM 8GB (GTX10180)\n",
    "\n",
    "# Kaggler like using tree-based model, (for tabular data) Gradient Boosting Decision Trees -> XGBoost, LightGBM, CatBoost\n",
    "# also, bagging-based RandomForest, ExtraTrees -> scilearn\n",
    "# (for DL models), PyTorch, TensorFlow, Keras\n",
    "# Sometimes, linear model has similar performance with tree-based model, if cannot loaded in RAM, then use Vowpal Wabbit\n",
    "# for hyperparameter optimizations, use hyperopt, scikit-optimize, spearmint\n",
    "# Baseline models means the basic ML pipline: to check if ML pipline correctly work\n",
    "# To fix the reproductivity, need to fix random_seed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python385jvsc74a57bd0aa8cfb42ece2888e792d42ad8b1a509016144ca8989462a2218cb8a3866bc65b"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}